###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\r\n\r\nclient<llm> Gemini2Flash {\r\n  provider google-ai\r\n  options {\r\n    model \"gemini-2.0-flash\"\r\n    api_key env.GEMINI_API_KEY\r\n    generationConfig {\r\n      temperature 0\r\n    }\r\n  }\r\n}\r\n\r\nclient<llm> Gemma3 {\r\n  provider openai-generic\r\n  retry_policy Exponential\r\n  options {\r\n    model \"google/gemma-3-27b-it:free\"\r\n    api_key env.OPENROUTER_API_KEY\r\n    base_url https://openrouter.ai/api/v1\r\n  }\r\n}\r\n\r\nclient<llm> GPT4oMini {\r\n  provider openai\r\n  retry_policy Exponential\r\n  options {\r\n    model \"gpt-4o-mini\"\r\n    api_key env.OPENAI_API_KEY\r\n    temperature 0\r\n  }\r\n}\r\n\r\nclient<llm> DeepSeek {\r\n  provider openai-generic\r\n  retry_policy Exponential\r\n  options {\r\n    model \"deepseek-chat\"\r\n    api_key env.DEEPSEEK_API_KEY\r\n    base_url https://api.deepseek.com\r\n    temperature 0 \r\n  }\r\n}\r\n\r\nclient<llm> QwQ3 {\r\n  provider openai-generic\r\n  retry_policy Exponential\r\n  options {\r\n    model \"qwen/qwen3-32b:free\"\r\n    api_key env.OPENROUTER_API_KEY\r\n    base_url https://openrouter.ai/api/v1\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\r\nclient<llm> CustomFast {\r\n  provider round-robin\r\n  options {\r\n    // This will alternate between the two clients\r\n    strategy [Gemini2Flash, GPT4oMini]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\r\nclient<llm> OpenaiFallback {\r\n  provider fallback\r\n  options {\r\n    // This will try the clients in order until one succeeds\r\n    strategy [Gemini2Flash, Gemma3]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/retry\r\nretry_policy Constant {\r\n  max_retries 3\r\n  // Strategy is optional\r\n  strategy {\r\n    type constant_delay\r\n    delay_ms 200\r\n  }\r\n}\r\n\r\nretry_policy Exponential {\r\n  max_retries 2\r\n  // Strategy is optional\r\n  strategy {\r\n    type exponential_backoff\r\n    delay_ms 300\r\n    multiplier 1.5\r\n    max_delay_ms 10000\r\n  }\r\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.87.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "memory.baml": "function MemoryAgent(user_prompt: string) -> string {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(\"\r\n    You are a helpful ai extractor assistant.\r\n    You will be given an interval of chat history between the customer and the assistant.\r\n    Your task is to extract the most important keywords to describe customer's insight and information, mainly using latest messages, into a string.\r\n    Please write in customer's language.\r\n    This string will be stored in database as long-term memory, so keep it short, concise and relevant.\r\n    \") }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"RESPONSE:\")) }}\r\n  \"#\r\n}\r\n\r\ntest TestMemoryAgent {\r\n  functions [MemoryAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "message_rewrite.baml": "\r\nfunction MessageRewriteAgent(user_prompt: string) -> string {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(\"\r\n    You are a helpful ai extractor assistant.\r\n    From customer's message, you will rewrite customer's needs as a string query in customer's language.\r\n    Fix the grammar and spelling errors.\r\n    Just rewrite the message, do not add any additional information.\r\n    Response in customer's language.\r\n    \") }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"RESPONSE:\")) }}\r\n  \"#\r\n}\r\n\r\n\r\n\r\n// Test the function with a sample resume. Open the VSCode playground to run this.\r\ntest TestMessageRewriteAgent {\r\n  functions [MessageRewriteAgent]\r\n  args {\r\n    user_prompt #\"\r\n      cho toi 5 sản phẩm trị mụn đang có sale tốt nhứtnhứt\r\n    \"#\r\n  }\r\n}\r\n",
    "script_retrieve.baml": "class ScriptRetrieveAgentOutput {\r\n    should_query_sheet bool\r\n    pieces_of_information string[]\r\n}\r\n\r\nfunction ScriptRetrieveAgent(dynamic_system_prompt: string,\r\n                        user_prompt: string, \r\n                        message_history: BAMLMessage[]\r\n                        ) -> ScriptRetrieveAgentOutput {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(\"\r\n    You are a helpful ai extractor assistant.\r\n    You will be provided with customer's message and a list of scripts.\r\n    Carefully study the scripts to define if we should query more from sheets.\r\n    Rerank and filter relevant information from the scripts.\r\n    Your response will contain the following:\r\n    - A boolean value indicating if we should query more from sheets.\r\n    - A list of pieces of information, each piece of information should be standalone script after rerank and filter. Each piece of information should be a detailed explanation of the customer's needs. Return at least 5 pieces of detailed information.\r\n    NOTE:\r\n    - If you set should_query_sheet to False, it means that you you decided information from scripts are enough to answer the customer's needs, and you need to provide detailed pieces of information that used to answer the customer's needs.\r\n    - If you set should_query_sheet to True, it means that you need to query more from sheets, and you need to provide detailed pieces of information that can instruct other agents to fulfill the customer's needs.\r\n    Response in customer's language.\r\n    \") }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintAssistantPrompt(dynamic_system_prompt)}}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n\r\n  \"#\r\n}\r\n\r\ntest TestScriptRetrieveAgent {\r\n  functions [ScriptRetrieveAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "sheet.baml": "template_string SheetSystemPrompt(dynamic_system_prompt: string) #\"\r\n    From the user's message and provided context, construct SQL query to query more information with sheet data.\r\n    Carefully study the user's message, the context of scripts, sheet columns to provide more information about the customer's needs.\r\n    From published sheets available from database, you need to analyze the sheets including their names, descriptions, especially column's type, description, example data. \r\n    Return the valid sql query.\r\n    {{ dynamic_system_prompt }}\r\n    Below are instructions for the you to write sql query:\r\n    Query from {sheet.table_name} when you know the table_name via the sheet you are querying. FROM \"{sheet.table_name}\" is the table name you need to query from.\r\n    You can use other fields of {table_name} specified in {sheet.column_config}, with normal operations to filter the query.\r\n    You should use &@~ instead of LIKE, ILIKE against string, text field of \"{sheet.table_name}\" to perform fulltext search, examine 'sheet.column_config' field of that sheet in 'sheets' table to know about these columns.\r\n    In SQL, the correct order of statement is:\r\n    SELECT …\r\n    FROM …\r\n    [WHERE …]\r\n    [GROUP BY …]\r\n    [HAVING …]\r\n    [ORDER BY …]\r\n    [LIMIT …];\r\n\r\n    NOTICE: The table to query FROM is \"{sheet.table_name}\".\r\n    Prefer using normal sql query to filter the data.\r\n    You must enclose the table name and column names in double quotes.\r\n    Carefully try different keywords when using LIKE, ILIKE for fulltext search.\r\n    Dont use single quotes for table name and column names.\r\n\r\n    For example:\r\n    SELECT\r\n      \"product_name\",\r\n      \"discounted_price\"\r\n    FROM \"some_table_name\"\r\n    WHERE \"product_price\" > 100\r\n\r\n    How to use PGroonga for text fields\r\n    &@~ operator\r\n    You can use &@~ operator to perform full text search by query syntax such as keyword1 OR keyword2:\r\n\r\n    SELECT * FROM memos WHERE content &@~ 'PGroonga OR PostgreSQL';\r\n    --  id |                            content\r\n    -- ----+----------------------------------------------------------------\r\n    --   3 | PGroonga is a PostgreSQL extension that uses Groonga as index.\r\n    --   1 | PostgreSQL is a relational database management system.\r\n    -- (2 rows)\r\n    Query syntax is similar to syntax of Web search engine ( keyword1 OR keyword2 means OR search and keyword1 keyword2 means AND search ). For example, you can use OR to merge result sets of performing full text search by two or more words. In the above example, you get a merged result set. The merged result set has records that includes PGroonga or PostgreSQL.\r\n    You must always enclose the pgroonga query in parentheses, for example: FROM \"some_table_name\" WHERE (\"data_fts\" &@~ 'a OR b')​\r\n\r\n    Example 1:\r\n    If the input keyword string is: 'tàn nhang'\r\n    Then the SQL query should be:\r\n    SELECT\r\n      \"product_name\",\r\n      \"discounted_price\"\r\n    FROM \"some_table_name\"\r\n    WHERE (\"description\" &@~ 'tàn nhang')\r\n\r\n    Normal query without fulltext search.\r\n    Example 2:\r\n    SELECT\r\n      \"product_name\",\r\n      \"discounted_price\"\r\n    FROM \"some_table_name\" as tb\r\n    WHERE tb.\"product_price\" > 100\r\n\"#\r\nclass SheetAgentOutput {\r\n    sql_query string\r\n}\r\nfunction SheetAgent(dynamic_system_prompt: string,\r\n                    user_prompt: string, \r\n                    message_history: BAMLMessage[]\r\n                    ) -> SheetAgentOutput {\r\n  client DeepSeek\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(SheetSystemPrompt(dynamic_system_prompt)) }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n  \"#\r\n}\r\n\r\ntest TestSheetAgent {\r\n  functions [SheetAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "sheet_rag.baml": "template_string SheetRAGSystemPrompt(dynamic_system_prompt: string) #\"\r\n    Know that previous SQL queries do not return any results.\r\n    From the user's message and provided context, construct RAG query more information with sheet data.\r\n    Carefully study the user's message, the context of scripts, sheet columns to provide more information about the customer's needs.\r\n    From published sheets available from database, you need to analyze the sheets including their names, descriptions, especially column's type, description, example data.\r\n    After analyzing the sheets, you can define the sheet_id, number of items and RAG query to use.\r\n    {{dynamic_system_prompt}}\r\n    NOTICE: You will query the RAG (Retrieval-Augmented Generation) database.\r\n    Your input query will be converted into sparse embeddings.\r\n    Sparse vector (BM25) captures exact keyword matches and term importance based on token frequency.\r\n    It is effective for precise matches on specific terms or phrases.\r\n    Each row of the knowledge sheet is represented as a vector in the RAG database\r\n    identified by 'sheet.id' in the 'sheet' table of database.\r\n\"#\r\nclass SheetRAGAgentOutput {\r\n    sheet_id string\r\n    limit int @description(#\"Maximum number of items to be returned from the RAG query\"#)\r\n    rag_query string\r\n}\r\nfunction SheetRAGAgent(dynamic_system_prompt: string,\r\n                    user_prompt: string,\r\n                    message_history: BAMLMessage[]\r\n                    ) -> SheetRAGAgentOutput {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(SheetRAGSystemPrompt(dynamic_system_prompt)) }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n  \"#\r\n}\r\n\r\ntest TestSheetRAGAgent {\r\n  functions [SheetRAGAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "synthetic.baml": "template_string SyntheticSystemPrompt(dynamic_system_prompt: string) #\"\r\n    You are Nguyen Thi Phuong Thao, a 25‑year‑old female customer service specialist at Mailisa Spa (skin care). You are dynamic, polite, supportive, well‑explained, and soft‑spoken. Always respond promptly with real data—never say you’ll “look up information later.”\r\n    You will response in customer's language, make the answer humanized, and use emojis to make the conversation more engaging. You are not a bot, so avoid using technical terms or jargon.\r\n    BEHAVIOR:\r\n    1. Think carefully before answering. If you don’t know after careful consideration, honestly say so and suggest contacting our HOTLINE 09001011 or visiting a Mailisa Spa location.\r\n    2. Always include accurate information provided—never guess or fabricate.\r\n    3. Provide extra useful insights (related products, skincare tips, promotions) to delight and guide the customer, but keep replies concise and focused.\r\n    4. You may share media via links when helpful.\r\n    5. Strongly focus on answering right pointedly and enoughly, just provide additional information if needed.\r\n    6. Do not FAKE the information.\r\n    7. Don't greet if already greeted.\r\n\r\n    GOAL:\r\n    Deliver accurate, engaging, and value‑added answers that showcase Mailisa’s expertise and encourage customers to book treatments or purchase products.\r\n    {{ dynamic_system_prompt }}\r\n\"#\r\nfunction SyntheticAgent(dynamic_system_prompt: string,\r\n                        user_prompt: string, \r\n                        message_history: BAMLMessage[]\r\n                        ) -> string {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(SyntheticSystemPrompt(dynamic_system_prompt)) }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"RESPONSE:\")) }}\r\n  \"#\r\n}\r\n\r\ntest TestSyntheticAgent {\r\n  functions [SyntheticAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "utils.baml": "class BAMLMessage {\r\n  role \"user\" | \"assistant\"\r\n  content string\r\n}\r\n\r\n// Inject a list of \"assistant\" or \"user\" messages into the prompt.\r\ntemplate_string PrintMessageHistory(messages: BAMLMessage[]) #\"\r\n  {% if messages|length > 0 %}\r\n    {% if \"notexist\" in ctx.client.name|lower %}\r\n      {% for m in messages %}\r\n        {% if m.role == \"user\" %}\r\n        <start_of_turn>user\r\n        {{ m.content }}<end_of_turn>\\n\r\n        {% else %}\r\n        <start_of_turn>model\r\n        {{ m.content }}<end_of_turn>\\n\r\n        {% endif %}\r\n      {% endfor %}\r\n    {% else %}\r\n      Here is the previous chat history:\r\n      {% for m in messages %}\r\n        {{ _.role(m.role) }}\r\n        {{ m.content }}\r\n      {% endfor %}\r\n    {% endif %}\r\n  {% endif %}\r\n\"#\r\n\r\ntemplate_string PrintSystemPrompt(system_prompt: string) #\"\r\n  {% if system_prompt %}\r\n    {% if \"notexist\" in ctx.client.name|lower %}\r\n      <start_of_turn>user\r\n      {{ system_prompt }}<end_of_turn>\\n\r\n    {% else %}\r\n      {{ _.role(\"system\") }}\r\n      {{ system_prompt }}\r\n    {% endif %}\r\n  {% endif %}\r\n\"#\r\n\r\ntemplate_string GetUserPrompt(user_prompt: string, response: string) #\"\r\n    ---------------\r\n    {{ user_prompt }}\r\n    ---------------\r\n\r\n    {{ ctx.output_format }}\r\n    {{ response }}\r\n\"#\r\n\r\ntemplate_string PrintUserPrompt(user_prompt: string) #\"\r\n  {% if user_prompt %}\r\n    {% if \"notexist\" in ctx.client.name|lower %}\r\n      <start_of_turn>user\r\n      Current customer's message:\r\n      {{ user_prompt }}<end_of_turn>\\n\r\n    {% else %}\r\n      {{ _.role(\"user\") }}\r\n      Current customer's message:\r\n      {{ user_prompt }}\r\n    {% endif %}\r\n  {% endif %}\r\n\"#\r\n\r\ntemplate_string PrintAssistantPrompt(assistant_prompt: string) #\"\r\n  {% if assistant_prompt %}\r\n    {% if \"notexist\" in ctx.client.name|lower %}\r\n      <start_of_turn>model\r\n      {{ assistant_prompt }}<end_of_turn>\\n\r\n    {% else %}\r\n      {{ _.role(\"assistant\") }}\r\n      {{ assistant_prompt }}\r\n    {% endif %}\r\n  {% endif %}\r\n\"#\r\n\r\n// function GenericQuery(systemPrompt: string, userPrompt: string) -> string {\r\n//   client Gemini2FlashLite\r\n\r\n//   prompt #\"\r\n//     {{ _.role(\"system\") }}\r\n\r\n//     {{ systemPrompt }}\r\n\r\n//     {{ _.role(\"user\") }}\r\n//     {{ userPrompt }}\r\n//   \"#\r\n// }\r\n\r\n// test generic_query {\r\n//   functions [GenericQuery]\r\n//   args {\r\n//     systemPrompt \"Anwser in the voice of Pikachu.\"\r\n//     userPrompt \"What's the capital of France?\"\r\n//   }\r\n// }\r\n\r\n// class GenericStructuredResponse {\r\n//   @@dynamic\r\n// }\r\n\r\n// function GenericStructuredOutputCall(systemPrompt: string, userPrompt: string, messages: Message[]) -> GenericStructuredResponse{\r\n//   client Gemini2FlashLite\r\n\r\n//   prompt #\"\r\n//     {{ _.role(\"system\") }}\r\n\r\n//     {{ systemPrompt }}\r\n\r\n//     {{ _.role(\"user\") }}\r\n//     {{ userPrompt }}\r\n\r\n//     {{ ctx.output_format }}\r\n//     Give step by step reasoning before giving the final answer in JSON format.\r\n//     Make sure to provide all fields required by the schema unless otherwise specified.\r\n\r\n//     {{ PrintMessages(messages) }}\r\n//   \"#\r\n// }\r\n\r\n// test generic_structured_output_call {\r\n//   functions [GenericStructuredOutputCall]\r\n//   args {\r\n//     systemPrompt \"Anwser in the voice of Pikachu.\"\r\n//     userPrompt \"What's the capital of France?\"\r\n//     messages [{\r\n//         role \"user\"\r\n//         message \"hello world\"\r\n//       },\r\n//       {\r\n//         role \"assistant\",\r\n//         message \"The capital of France is Paris.\"\r\n//       }\r\n//     ]\r\n//   }\r\n// }",
}

def get_baml_files():
    return file_map
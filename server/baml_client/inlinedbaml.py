###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\r\n\r\nclient<llm> Gemini2Flash {\r\n  provider google-ai\r\n  options {\r\n    model \"gemini-2.0-flash\"\r\n    api_key env.GEMINI_API_KEY\r\n    generationConfig {\r\n      temperature 0\r\n    }\r\n  }\r\n}\r\n\r\nclient<llm> Gemini2FlashLite {\r\n  provider google-ai\r\n  options {\r\n    model \"gemini-2.0-flash-lite\"\r\n    api_key env.GEMINI_API_KEY\r\n    generationConfig {\r\n      temperature 0\r\n    }\r\n  }\r\n}\r\n\r\nclient<llm> Gemma3 {\r\n  provider openai-generic\r\n  retry_policy Exponential\r\n  options {\r\n    model \"google/gemma-3-27b-it:free\"\r\n    api_key env.OPENROUTER_API_KEY\r\n    base_url https://openrouter.ai/api/v1\r\n    temperature 0\r\n  }\r\n}\r\n\r\nclient<llm> GPT4oMini {\r\n  provider openai\r\n  retry_policy Exponential\r\n  options {\r\n    model \"gpt-4o-mini\"\r\n    api_key env.OPENAI_API_KEY\r\n    temperature 0\r\n  }\r\n}\r\n\r\nclient<llm> DeepSeek {\r\n  provider openai-generic\r\n  retry_policy Exponential\r\n  options {\r\n    model \"deepseek-chat\"\r\n    api_key env.DEEPSEEK_API_KEY\r\n    base_url https://api.deepseek.com\r\n    temperature 0 \r\n  }\r\n}\r\n\r\nclient<llm> QwQ3 {\r\n  provider openai-generic\r\n  retry_policy Exponential\r\n  options {\r\n    model \"qwen/qwen3-30b-a3b:free\"\r\n    api_key env.OPENROUTER_API_KEY\r\n    base_url https://openrouter.ai/api/v1\r\n    temperature 0\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\r\nclient<llm> CustomFast {\r\n  provider round-robin\r\n  options {\r\n    // This will alternate between the two clients\r\n    strategy [Gemini2Flash, GPT4oMini]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\r\nclient<llm> OpenaiFallback {\r\n  provider fallback\r\n  options {\r\n    // This will try the clients in order until one succeeds\r\n    strategy [Gemini2FlashLite,GPT4oMini]\r\n  }\r\n}\r\n\r\n// https://docs.boundaryml.com/docs/snippets/clients/retry\r\nretry_policy Constant {\r\n  max_retries 3\r\n  // Strategy is optional\r\n  strategy {\r\n    type constant_delay\r\n    delay_ms 200\r\n  }\r\n}\r\n\r\nretry_policy Exponential {\r\n  max_retries 2\r\n  // Strategy is optional\r\n  strategy {\r\n    type exponential_backoff\r\n    delay_ms 300\r\n    multiplier 1.5\r\n    max_delay_ms 10000\r\n  }\r\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.88.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "memory.baml": "function MemoryAgent(user_prompt: string) -> string {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(\"\r\n    You are a helpful ai extractor assistant.\r\n    You will be given an interval of chat history between the customer and the assistant.\r\n    Your task is to extract the most important keywords to describe customer's insight and information, mainly using latest messages, into a string.\r\n    Please write in customer's language.\r\n    This string will be stored in database as long-term memory, so keep it short, concise and relevant.\r\n    \") }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"RESPONSE:\")) }}\r\n  \"#\r\n}\r\n\r\ntest TestMemoryAgent {\r\n  functions [MemoryAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "message_rewrite.baml": "\r\nfunction MessageRewriteAgent(user_prompt: string) -> string {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(\"\r\n    You are a helpful ai extractor assistant.\r\n    From customer's message, you will rewrite customer's needs as a string query in customer's language.\r\n    Fix the grammar and spelling errors.\r\n    Just rewrite the message, do not add any additional information.\r\n    Response in customer's language.\r\n    \") }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"RESPONSE:\")) }}\r\n  \"#\r\n}\r\n\r\n\r\n\r\n// Test the function with a sample resume. Open the VSCode playground to run this.\r\ntest TestMessageRewriteAgent {\r\n  functions [MessageRewriteAgent]\r\n  args {\r\n    user_prompt #\"\r\n      cho toi 5 sản phẩm trị mụn đang có sale tốt nhứtnhứt\r\n    \"#\r\n  }\r\n}\r\n",
    "script_retrieve.baml": "class ScriptRetrieveAgentOutput {\r\n    should_query_sheet bool\r\n    pieces_of_information string[]\r\n}\r\n\r\nfunction ScriptRetrieveAgent(dynamic_system_prompt: string,\r\n                        user_prompt: string, \r\n                        message_history: BAMLMessage[]\r\n                        ) -> ScriptRetrieveAgentOutput {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(\"\r\n    You are a helpful ai extractor assistant.\r\n    You will be provided with customer's message and a list of scripts.\r\n    Carefully study the scripts and the description of each sheet to define if we should query more from sheets.\r\n    Rerank and filter relevant information from the scripts.\r\n    Your response will contain the following:\r\n    - A boolean value indicating if we should query more from sheets.\r\n    - A list of pieces of information, each piece of information should be standalone script after rerank and filter. Each piece of information should be a detailed explanation of the customer's needs.\r\n    NOTE:\r\n    - If you think information from scripts are enough to answer the customer's needs, OR any information instruct you to prompt more from customers to clarify their needs, so set should_query_sheet to False\r\n    - If you think that customer've provided enough condition data for query sheet and should query sheet for for information, set should_query_sheet to True.    \r\n    if you need to prompt more from customer, please provide a list of pieces of information that you need to ask the customer.\r\n    Must not contain information that you can see in the previous chat history. Just contain new information.\r\n    Response in customer's language.\r\n    \") }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintAssistantPrompt(dynamic_system_prompt)}}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n    {{ OutputStructureChainOfThought() }}\r\n  \"#\r\n}\r\n\r\ntest TestScriptRetrieveAgent {\r\n  functions [ScriptRetrieveAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "sheet.baml": "template_string SheetSystemPrompt(dynamic_system_prompt: string) #\"\r\n    From the user's message and provided context, construct SQL query to query more information with sheet data.\r\n    Carefully study the user's message, the context of scripts, sheet columns to provide more information about the customer's needs.\r\n    From published sheets available from database, you need to analyze the sheets including their names, descriptions, especially column's type, description. \r\n    Dont query too much information from sheets, but also dont make customer to re ask many times.\r\n    Return the valid sql query.\r\n    {{ dynamic_system_prompt }}\r\n    Below are instructions for the you to write sql query:\r\n    Query from {sheet.table_name} when you know the table_name via the sheet you are querying. FROM \"{sheet.table_name}\" is the table name you need to query from.\r\n    You can use other fields of {table_name} specified in {sheet.column_config}, with normal operations to filter the query.\r\n    You should use &@~ instead of = ,LIKE, ILIKE against string, text field of \"{sheet.table_name}\" to perform fulltext search, examine 'sheet.column_config' field of that sheet in 'sheets' table to know about these columns.\r\n    In SQL, the correct order of statement is:\r\n    SELECT …\r\n    FROM …\r\n    [WHERE …]\r\n    [GROUP BY …]\r\n    [HAVING …]\r\n    [ORDER BY …]\r\n    [LIMIT …];\r\n\r\n    NOTICE: The table to query FROM is \"{sheet.table_name}\".\r\n    Prefer using normal sql query to filter the data.\r\n    You must enclose the table name and column names in double quotes.\r\n    Carefully try different keywords when using LIKE, ILIKE for fulltext search.\r\n    Dont use single quotes for table name and column names.\r\n\r\n    For example:\r\n    SELECT\r\n      \"product_name\",\r\n      \"discounted_price\"\r\n    FROM \"some_table_name\"\r\n    WHERE \"product_price\" > 100\r\n\r\n    How to use PGroonga for text fields\r\n    &@~ operator\r\n    You can use &@~ operator to perform full text search by query syntax such as keyword1 OR keyword2:\r\n\r\n    SELECT * FROM memos WHERE content &@~ '\"PGroonga\" OR \"PostgreSQL\"';\r\n    --  id |                            content\r\n    -- ----+----------------------------------------------------------------\r\n    --   3 | PGroonga is a PostgreSQL extension that uses Groonga as index.\r\n    --   1 | PostgreSQL is a relational database management system.\r\n    -- (2 rows)\r\n    Query syntax is similar to syntax of Web search engine ( \"keyword1\" OR \"keyword2\" means OR search and \"keyword1\" \"keyword2\" means AND search ). For example, you can use OR to merge result sets of performing full text search by two or more words. In the above example, you get a merged result set. The merged result set has records that includes \"PGroonga\" or \"PostgreSQL\".\r\n    You must always enclose the pgroonga query in parentheses, for example: FROM \"some_table_name\" WHERE (\"data_fts\" &@~ 'a OR b')​\r\n\r\n    Example 1:\r\n    If the input keyword string is: 'tàn nhang'\r\n    Then the SQL query should be:\r\n    SELECT\r\n      \"product_name\",\r\n      \"discounted_price\"\r\n    FROM \"some_table_name\"\r\n    WHERE (\"description\" &@~ '\"tàn nhang\"')\r\n\r\n    Normal query without fulltext search.\r\n    Example 2:\r\n    SELECT\r\n      \"product_name\",\r\n      \"discounted_price\"\r\n    FROM \"some_table_name\" as tb\r\n    WHERE tb.\"product_price\" > 100\r\n\r\n    NOTE: Also return the sheet_id and limit for the RAG vector database query if not found any data by sql_query.\r\n\"#\r\nclass SheetAgentOutput {\r\n    sql_query string @description(\"The valid sql query to query from the sheet.\")\r\n    sheet_id string @description(\"The sheet_id to query from.\")\r\n    limit int @description(\"The maximum number of items to return by using RAG.\")\r\n}\r\nfunction SheetAgent(dynamic_system_prompt: string,\r\n                    user_prompt: string, \r\n                    message_history: BAMLMessage[]\r\n                    ) -> SheetAgentOutput {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(SheetSystemPrompt(dynamic_system_prompt)) }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n  \"#\r\n}\r\n\r\ntest TestSheetAgent {\r\n  functions [SheetAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "sheet_guard.baml": "template_string SheetGuardSystemPrompt(dynamic_system_prompt: string) #\"\r\n    You are a helpful ai routing assistant.\r\n    Read current customer's message, previous history and sheet information.\r\n    Carefully study the user's message, the context of scripts, sheet columns to provide more information about the customer's needs.\r\n    From published sheets available from database, you need to analyze the sheets including their names, descriptions, especially column's type, description.\r\n    Determine if we could find more helpful information by querying more from sheets, OR send these information to next agent who will answer customer's question.\r\n    Dont make too frequent queries to sheets, but also dont make customer to re ask many times.\r\n    \r\n\"#\r\n\r\nclass SheetGuardAgentOutput{\r\n    should_query_sheet bool\r\n}\r\n\r\nfunction SheetGuardAgent(dynamic_system_prompt: string,\r\n                    user_prompt: string,\r\n                    message_history: BAMLMessage[]\r\n                    ) -> SheetGuardAgentOutput {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(SheetGuardSystemPrompt(dynamic_system_prompt)) }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintAssistantPrompt(dynamic_system_prompt) }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n    {{ CommonChainOfThoughts() }}\r\n  \"#\r\n}\r\n\r\ntest TestSheetGuardAgent {\r\n  functions [SheetGuardAgent]\r\n  args {\r\n    dynamic_system_prompt \"Provided items: bộ B17, có giá là 1 triệu đồng, có khuyến mãi 20% cho khách hàng mới, giúp dưỡng trắng da tay chân và da body cực tốt\"\r\n    user_prompt #\"\r\n      tôi cần mua sản phẩm N3\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "sheet_rag.baml": "template_string SheetRAGSystemPrompt(dynamic_system_prompt: string) #\"\r\n    Know that previous SQL queries do not return any results.\r\n    From the user's message and provided context, construct RAG query more information with sheet data.\r\n    Carefully study the user's message, the context of scripts, sheet columns to provide more information about the customer's needs.\r\n    From published sheets available from database, you need to analyze the sheets including their names, descriptions, especially column's type, description, example data.\r\n    After analyzing the sheets, you can define the sheet_id, number of items and RAG query to use.\r\n    {{dynamic_system_prompt}}\r\n    NOTICE: You will query the RAG (Retrieval-Augmented Generation) database.\r\n    Your input query will be converted into sparse embeddings.\r\n    Sparse vector (BM25) captures exact keyword matches and term importance based on token frequency.\r\n    It is effective for precise matches on specific terms or phrases.\r\n    Each row of the knowledge sheet is represented as a vector in the RAG database\r\n    identified by 'sheet.id' in the 'sheet' table of database.\r\n\"#\r\nclass SheetRAGAgentOutput {\r\n    sheet_id string\r\n    limit int @description(#\"Maximum number of items to be returned from the RAG query\"#)\r\n    rag_query string\r\n}\r\nfunction SheetRAGAgent(dynamic_system_prompt: string,\r\n                    user_prompt: string,\r\n                    message_history: BAMLMessage[]\r\n                    ) -> SheetRAGAgentOutput {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(SheetRAGSystemPrompt(dynamic_system_prompt)) }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n  \"#\r\n}\r\n\r\ntest TestSheetRAGAgent {\r\n  functions [SheetRAGAgent]\r\n  args {\r\n    dynamic_system_prompt \"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới.\"\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "sheet_rag_guard.baml": "template_string SheetRAGGuardSystemPrompt(dynamic_system_prompt: string) #\"\r\n    Read current customer's message, previous history and provided list of items.\r\n    Carefully study the data to give information that can help to answer the customer's need.\r\n    You must base on the relevance of customer's message and provided items to determine if suggest or not.\r\n    If you cannot see any helpful information, please say \"I don't know\" and do not provide any information.\r\n    {{dynamic_system_prompt}}\r\n    {{ CommonChainOfThoughts() }}\r\n    Response in customer's language.\r\n\"#\r\n\r\nfunction SheetRAGGuardAgent(dynamic_system_prompt: string,\r\n                    user_prompt: string,\r\n                    message_history: BAMLMessage[]\r\n                    ) -> string {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(SheetRAGGuardSystemPrompt(dynamic_system_prompt)) }}\r\n    \r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n  \"#\r\n}\r\n\r\ntest TestSheetRAGGuardAgent {\r\n  functions [SheetRAGGuardAgent]\r\n  args {\r\n    dynamic_system_prompt \"Provided items: bộ B17, có giá là 1 triệu đồng, có khuyến mãi 20% cho khách hàng mới, giúp dưỡng trắng da tay chân và da body cực tốt\"\r\n    user_prompt #\"\r\n      tôi cần mua sản phẩm N3\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "synthetic.baml": "template_string SyntheticSystemPrompt(dynamic_system_prompt: string) #\"\r\n    You are Nguyen Thi Phuong Thao, a 25‑year‑old female customer service specialist at Mailisa Spa (skin care). You are dynamic, polite, supportive, well‑explained, and soft‑spoken. Always respond promptly with real data—never say you’ll “look up information later.”\r\n    You will response in customer's language, make the answer humanized, and use emojis to make the conversation more engaging. You are not a bot, so avoid using technical terms or jargon.\r\n    BEHAVIOR:\r\n    1. Just provide information from trusted context. If not sure, honestly say so and suggest contacting our HOTLINE 0932 699 299 - 0931 699 299 or visiting a Mailisa Spa location.\r\n    2. Sometimes you can ask the customer to provide more information if you think it is necessary to answer the customer's needs.\r\n    3. Provide extra useful insights (related ones, tips, promotions) to delight and guide the customer, but keep replies concise and focused.\r\n    4. Limit the length of response to 4096 characters. Must break response into multiple messages if the response is too long, but group relevant messages to make consistent.\r\n    5. Strongly focus on answering right pointedly and enoughly, just provide additional information if needed.\r\n    6. Don't greet if already greeted. Don't repeat information that has already been provided. Don't repeat the same information in different ways.\r\n    7. Limit the use of emojis to 2-3 per message. Use them to enhance the message, not to clutter it.\r\n    GOAL:\r\n    Deliver trusted, accurate, engaging, and value‑added answers that showcase Mailisa’s expertise and encourage customers to book treatments or purchase products.\r\n\"#\r\nclass ChatResponseItem {\r\n  type \"text\" | \"image\" | \"video\" | \"audio\" | \"file\" | \"link\" @description(#\"\r\n    The type of the content. The type can be one of the following:\r\n    - \"text\": The content is a text message.\r\n    - \"image\": The content is a URL to an image.\r\n    - \"video\": The content is a URL to a video.\r\n    - \"audio\": The content is a URL to an audio file.\r\n    - \"file\": The content is a URL to a file.\r\n    - \"link\": The content is a URL to a link. If a response item is a \"link\" type, the previous response should be \"text\" type with description for this link.\r\n  \"#)\r\n  payload string @description(#\"\r\n    The content of the response. The type of the content is determined by the type field.\r\n    The content is a string if the type is \"text\".\r\n    The content is a URL if the type is \"image\", \"video\", \"audio\", or \"file\".\r\n  \"#)\r\n}\r\n\r\nfunction SyntheticAgent(dynamic_system_prompt: string,\r\n                        user_prompt: string, \r\n                        message_history: BAMLMessage[]\r\n                        ) ->  ChatResponseItem[] {\r\n  client OpenaiFallback\r\n  prompt #\"\r\n    {{ PrintSystemPrompt(SyntheticSystemPrompt(dynamic_system_prompt)) }}\r\n    {{ PrintMessageHistory(message_history) }}\r\n    {{ PrintAssistantPrompt(dynamic_system_prompt)}}\r\n    \r\n    {{ PrintUserPrompt(GetUserPrompt(user_prompt, \"JSON:\")) }}\r\n    {{ OutputStructureChainOfThought() }}\r\n  \"#\r\n}\r\n\r\ntest TestSyntheticAgent {\r\n  functions [SyntheticAgent]\r\n  args {\r\n    dynamic_system_prompt #\"Khi khách hàng cần mua hàng, hãy truy vấn Bảng sản phẩm để lấy giá, đồng thời giới thiệu khuyến mãi 20% cho khách hàng mới. \r\n    Giá một bộ M66 là 1.000.000 VNĐ. Hãy yêu cầu cung cấp thông tin địa chỉ và số điện thoại của khách hàng để xác nhận đơn hàng. Link ảnh M66 là https://mailisa.com/wp-content/uploads/2024/10/M66-1.jpg.webp\r\n    \"#\r\n    user_prompt #\"\r\n      tôi cần mua 1 bộ M66\r\n    \"#\r\n    message_history [{\r\n      role \"user\"\r\n      content \"xin chào\"\r\n    },\r\n    {\r\n      role \"assistant\",\r\n      content \"Chào bạn, tôi có thể giúp gì cho bạn?\"\r\n    }]\r\n  }\r\n}\r\n",
    "utils.baml": "class BAMLMessage {\r\n  role \"user\" | \"assistant\"\r\n  content string\r\n}\r\n\r\n// Inject a list of \"assistant\" or \"user\" messages into the prompt.\r\ntemplate_string PrintMessageHistory(messages: BAMLMessage[]) #\"\r\n  {% if messages|length > 0 %}\r\n    {% if \"notexist\" in ctx.client.name|lower %}\r\n      {% for m in messages %}\r\n        {% if m.role == \"user\" %}\r\n        <start_of_turn>user\r\n        {{ m.content }}<end_of_turn>\\n\r\n        {% else %}\r\n        <start_of_turn>model\r\n        {{ m.content }}<end_of_turn>\\n\r\n        {% endif %}\r\n      {% endfor %}\r\n    {% else %}\r\n      Here is the previous chat history:\r\n      {% for m in messages %}\r\n        {{ _.role(m.role) }}\r\n        {{ m.content }}\r\n      {% endfor %}\r\n    {% endif %}\r\n  {% endif %}\r\n\"#\r\n\r\ntemplate_string PrintSystemPrompt(system_prompt: string) #\"\r\n  {% if system_prompt %}\r\n    {% if \"notexist\" in ctx.client.name|lower %}\r\n      <start_of_turn>user\r\n      {{ system_prompt }}<end_of_turn>\\n\r\n    {% else %}\r\n      {{ _.role(\"system\") }}\r\n      {{ system_prompt }}\r\n    {% endif %}\r\n  {% endif %}\r\n\"#\r\n\r\ntemplate_string GetUserPrompt(user_prompt: string, response: string) #\"\r\n    ---------------\r\n    {{ user_prompt }}\r\n    ---------------\r\n\r\n    {{ ctx.output_format }}\r\n    {{ response }}\r\n\"#\r\n\r\ntemplate_string PrintUserPrompt(user_prompt: string) #\"\r\n  {% if user_prompt %}\r\n    {% if \"notexist\" in ctx.client.name|lower %}\r\n      <start_of_turn>user\r\n      Current customer's message:\r\n      {{ user_prompt }}<end_of_turn>\\n\r\n    {% else %}\r\n      {{ _.role(\"user\") }}\r\n      \\n\\nCurrent customer's message:\r\n      {{ user_prompt }}\r\n    {% endif %}\r\n  {% endif %}\r\n\"#\r\n\r\ntemplate_string PrintAssistantPrompt(assistant_prompt: string) #\"\r\n  {% if assistant_prompt %}\r\n    {% if \"notexist\" in ctx.client.name|lower %}\r\n      <start_of_turn>model\r\n      {{ assistant_prompt }}<end_of_turn>\\n\r\n    {% else %}\r\n      {{ _.role(\"assistant\") }}\r\n      {{ assistant_prompt }}\r\n    {% endif %}\r\n  {% endif %}\r\n\"#\r\n\r\ntemplate_string CommonChainOfThoughts() #\"\r\n  Outline some relevant information before you answer.\r\n    Example:\r\n    - ...\r\n    - ...\r\n    ...\r\n    {\r\n      ... // schema\r\n    }\r\n\"#\r\n\r\ntemplate_string OutputStructureChainOfThought(action: string?) #\"\r\n    Before you answer, please explain your reasoning step-by-step.\r\n    {% if action %}{{ action }}{% endif %}\r\n    \r\n    For example:\r\n    If we think step by step we can see that ...\r\n    Therefore the output is:\r\n    {\r\n      ... // schema\r\n    }\r\n\"#\r\n\r\n// function GenericQuery(systemPrompt: string, userPrompt: string) -> string {\r\n//   client Gemini2FlashLite\r\n\r\n//   prompt #\"\r\n//     {{ _.role(\"system\") }}\r\n\r\n//     {{ systemPrompt }}\r\n\r\n//     {{ _.role(\"user\") }}\r\n//     {{ userPrompt }}\r\n//   \"#\r\n// }\r\n\r\n// test generic_query {\r\n//   functions [GenericQuery]\r\n//   args {\r\n//     systemPrompt \"Anwser in the voice of Pikachu.\"\r\n//     userPrompt \"What's the capital of France?\"\r\n//   }\r\n// }\r\n\r\n// class GenericStructuredResponse {\r\n//   @@dynamic\r\n// }\r\n\r\n// function GenericStructuredOutputCall(systemPrompt: string, userPrompt: string, messages: Message[]) -> GenericStructuredResponse{\r\n//   client Gemini2FlashLite\r\n\r\n//   prompt #\"\r\n//     {{ _.role(\"system\") }}\r\n\r\n//     {{ systemPrompt }}\r\n\r\n//     {{ _.role(\"user\") }}\r\n//     {{ userPrompt }}\r\n\r\n//     {{ ctx.output_format }}\r\n//     Give step by step reasoning before giving the final answer in JSON format.\r\n//     Make sure to provide all fields required by the schema unless otherwise specified.\r\n\r\n//     {{ PrintMessages(messages) }}\r\n//   \"#\r\n// }\r\n\r\n// test generic_structured_output_call {\r\n//   functions [GenericStructuredOutputCall]\r\n//   args {\r\n//     systemPrompt \"Anwser in the voice of Pikachu.\"\r\n//     userPrompt \"What's the capital of France?\"\r\n//     messages [{\r\n//         role \"user\"\r\n//         message \"hello world\"\r\n//       },\r\n//       {\r\n//         role \"assistant\",\r\n//         message \"The capital of France is Paris.\"\r\n//       }\r\n//     ]\r\n//   }\r\n// }",
}

def get_baml_files():
    return file_map